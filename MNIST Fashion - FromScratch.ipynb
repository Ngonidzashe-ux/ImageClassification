{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3eac054",
   "metadata": {},
   "source": [
    "# MNIST FASHION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53214340",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 11:07:39.492649: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/ngoni/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Conv2D, Dense, Flatten, Dropout, MaxPool2D\n",
    "from keras.models import Model\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d294e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Fashion Mnist Data Set\n",
    "mnist_images = keras.datasets.fashion_mnist\n",
    "\n",
    "# Dividing into training set and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist_images.load_data()\n",
    "\n",
    "# Normalizing the dataset by dividing pixel values by 255.0\n",
    "X_train, X_test = X_train/255.0, X_test/255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4591991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the image for visualization using matplotlib\n",
    "image_data = X_test[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8834d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create a new window/container/figure to draw in\n",
    "plt.figure()\n",
    "\n",
    "#displays the image located at index 5 of the X test array\n",
    "plt.imshow(image_data)\n",
    "\n",
    "#adds a colour bar to the plot\n",
    "plt.colorbar()\n",
    "\n",
    "#Turns off grid lines\n",
    "plt.grid(False)\n",
    "\n",
    "#Displays entire plot or entire visualization and without it\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1153393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You need to reshape the image into something similar to an actual photograph using cv2. .reshape(height, width, channels.)\n",
    "#RGB = 3 CHANNELS AND grey = one channel\n",
    "image = image_data.reshape(28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a06b3ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReshaped Image\u001b[39m\u001b[38;5;124m'\u001b[39m, image)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#cv2.waitKey(delay): This is for leaving the image window open until a key is pressed. After the key is pressed,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#it waits for the delay(millseconds) until it closes the window\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#used to destroy all OpenCV windows that are currently open\u001b[39;00m\n\u001b[1;32m      8\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reshaping the image using OpenCV to display in a window\n",
    "image = image_data.reshape(28, 28, 1)\n",
    "cv2.imshow('Reshaped Image', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0abd05b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Adding color channel axes to the images in the training and test sets\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de8e1324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "#debugging\n",
    "print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "461e4531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "#label encoding\n",
    "#labels are represented as labels and set extracts the unique labels from y_train\n",
    "#y_train is one dimensional array containing the labels of all the information\n",
    "#There should be a key else where to determine what each label correspond to.\n",
    "outputs = set(y_train)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46d67b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model architecture using Convolutional Neural Network (CNN)\n",
    "\n",
    "# Input layer that takes the shape of a single image in the dataset\n",
    "inp_layer = Input(shape=X_train[0].shape)\n",
    "\n",
    "# First Convolutional layer with 32 filters of size 3x3 and a stride of 2\n",
    "# The activation function 'relu' introduces non-linearity\n",
    "conv1 = Conv2D(32, (3,3), strides=2, activation='relu')(inp_layer)\n",
    "\n",
    "# MaxPooling layer to downsample the spatial dimensions by 2x2\n",
    "maxp1 = MaxPool2D(2, 2)(conv1)\n",
    "\n",
    "# Second Convolutional layer with 64 filters of size 3x3 and a stride of 2\n",
    "# Activation function 'relu' is applied for non-linearity\n",
    "conv2 = Conv2D(64, (3,3), strides=2, activation='relu')(maxp1)\n",
    "\n",
    "# Flatten layer to convert the multi-dimensional tensor into a one-dimensional array\n",
    "flat1 = Flatten()(conv2)\n",
    "\n",
    "# First Dense (Fully Connected) layer with 256 neurons and 'relu' activation\n",
    "dense1 = Dense(256, activation='relu')(flat1)\n",
    "\n",
    "# Second Dense (Fully Connected) layer with 128 neurons and 'relu' activation\n",
    "dense2 = Dense(128, activation='relu')(dense1)\n",
    "\n",
    "# Output layer with the number of neurons equal to the number of unique labels in the dataset\n",
    "# 'softmax' activation is used for multi-class classification\n",
    "out_layer = Dense(len(set(y_train)), activation='softmax')(dense2)\n",
    "\n",
    "# Creating the model using the functional API\n",
    "model_func = Model(inp_layer, out_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45444e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model with Adam optimizer and sparse categorical crossentropy loss\n",
    "model_func.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Displaying the model architecture\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model_func, to_file='model_plot.png', show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a027c0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3750/3750 [==============================] - 13s 3ms/step - loss: 0.5256 - accuracy: 0.8051 - val_loss: 0.4318 - val_accuracy: 0.8369\n",
      "Epoch 2/5\n",
      "3750/3750 [==============================] - 10s 3ms/step - loss: 0.3776 - accuracy: 0.8594 - val_loss: 0.3728 - val_accuracy: 0.8654\n",
      "Epoch 3/5\n",
      "3750/3750 [==============================] - 12s 3ms/step - loss: 0.3363 - accuracy: 0.8744 - val_loss: 0.3484 - val_accuracy: 0.8692\n",
      "Epoch 4/5\n",
      "3750/3750 [==============================] - 11s 3ms/step - loss: 0.3063 - accuracy: 0.8855 - val_loss: 0.3516 - val_accuracy: 0.8746\n",
      "Epoch 5/5\n",
      "3750/3750 [==============================] - 11s 3ms/step - loss: 0.2843 - accuracy: 0.8932 - val_loss: 0.3311 - val_accuracy: 0.8789\n"
     ]
    }
   ],
   "source": [
    "# Training the model on the provided data for 5 epochs\n",
    "history = model_func.fit(X_train,\n",
    "y_train,\n",
    "epochs=5,\n",
    "batch_size=16,\n",
    "validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ba61a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
