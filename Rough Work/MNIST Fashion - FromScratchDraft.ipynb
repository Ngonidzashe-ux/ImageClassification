{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c76962",
   "metadata": {},
   "source": [
    "# MNIST FASHION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53214340",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 11:07:55.989773: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/ngoni/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Conv2D, Dense, Flatten, Dropout, MaxPool2D\n",
    "from keras.models import Model\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d294e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Fashion Mnist Data Set\n",
    "mnist_images = keras.datasets.fashion_mnist\n",
    "\n",
    "#Divide into training set and test sets\n",
    "#.load_data returns two tuples. First one contains an array of images(matrices) for training and second one is an array of images for testing\n",
    "#Each tuple contains two elements, first is the array of images and second is that of corresponding labels\n",
    "#X_train, X_test: These typically represent the feature matrices of the training and test datasets, respectively. \"X\" usually denotes input features.\n",
    "#y_train, y_test: These represent the corresponding labels or target values for the training and test datasets. \"y\" usually denotes output labels.\n",
    "(X_train, y_train), (X_test, y_test) = mnist_images.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7951794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5206ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade --force-reinstall tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c24a6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6da717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The images in the fashion MNIST dataset are greyscale where each pixel value is between 0 and 255. To normalise the dataset, the MNIST image pixels are divided by 255.0.\n",
    "X_train, X_test = X_train/255.0, X_test/255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99dbdc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#returns the following tuple(number of images, image width in terms of number of pixels, image height)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4591991",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = X_test[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8834d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS4AAAD8CAYAAADJwUnTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAZ40lEQVR4nO3de5CcV3nn8e9vbhrdZVm2LEsyNl5hY6jYsFoLyrAr4zXIrrCG3ZBY7BKHgghvoV2oyh/2shdTtbVV3iRAnFoH7QCK7VpiQwUTFCJQiCHYSbBX8k2WLNtoZcUaS9YFoftlZnqe/aNbuKdn3tM9Mz3T7zv6faq6Zvp93suZ1uiZc8573nMUEZiZFUlbqwtgZjZaTlxmVjhOXGZWOE5cZlY4TlxmVjhOXGZWOE5cZjZhJK2XdEDStoy4JP2xpJ2Stkp6dyPndeIys4n0ALAqEb8FWFZ5rQG+2shJnbjMbMJExOPA4cQutwEPRdmTwDxJi+qdt6NZBWxEl6ZFNzMn85JTQtfV6b8vgygzVhpMH9veNpiMn+rrSsbb2tJPXnR39GfG+gfbk8eK9Lmzf+qyeCX72lPVGU7SF2frfTRJH7pxZvzicKmhfZ/eenY7cKZqU09E9IzicouBPVXveyvb9qUOGlfikrQKuA9oB74eEfem9u9mJit003gueV669MHZyfjpUmdm7Fhfd/LYeV2nk/FnXl+SjE+flk4OVy04kBnbfyr9c3W1pf/z1Eu6pRv3JuNT0VPx2LjPcehwiac2pf/dz+lc9P/ORMTycVxupCRb9znEMScuSe3A/cDNlLPkZkkbIuLFsZ7TzPIgKEX6j0IT9QJLq94vAer+xRlPH9f1wM6I2BURfcAjlNurZlZgAQwSDb2aYAPw25W7i+8BjkZEspkI42sqjtQ2XVG7k6Q1lO8W0M2McVzOzCbLIM2pcUl6GFgJLJDUC9wDdAJExDpgI3ArsBM4BXyykfOOJ3E11DatdNT1AMzRfM+hY5ZzQdDfpKZiRKyuEw/gs6M973gS15japmaWbwGUmtMMnDDj6ePaDCyTdIWkLuB2yu1VMyu4SezjGpMx17giYkDSWmAT5eEQ6yNie9NKdh5pX3BhMr524Q+S8X84tWzM176085fJ+O0XP5WMHxyYk4yfieyhGvtmzkseO7/jZDK+5ehbkvFfJKOWJYBSzmdGHtc4rojYSLlzzcymkEkbDDFGkzpy3szyL4jc93E5cZnZEBHQn++85cRlZrVEqe6ToK3lxGVmQwQw6BqXmRWNa1xmVijlAahOXFaHZqaf4az3SzSj7Wxm7Ggpfe4jdeIvnro0GZ/WNpCMX9mdPa3NYKR/rk37r0nGj55NT9kzl/QYNRtZAP2R7zlGnbjMbIhAlHI+ObITl5kNU6823GpOXGY2hPu4zKyARMl9XGZWJOUZUJ24zKxAIkRfpFdgajUnrhzY/fGlyfi16RXC+Ktj2dPizGjrSx7bqfRwhiP96eESHXVW4rm4K3vIwoz2dNkum5UezrDwwmPJ+M8+cH1mrOPHTyePPd+llrzLAycuMxui3DnvpqKZFYo7582sYNw5b2aFVPIAVDMrkkD0R75TQ75LZ2aTzp3zZlY4gdxUtPo++ltPJOM/PZ0eS/X8kSWZsXfN25M89tTgtGT85gvSK869MTA3GT87mL082aH+WcljT5eyjwV436xXkvHvrnh/ZmzJj5OHnvfcOW9mhRKBh0OYWbGUO+f9yI+ZFYw7582sUAJ5IkEzKx7XuMysUMrrKjpxmVmheCVra8Cd83+WjP/Xvbck4xdOO5kZm9txKnlsp9Lzae3pn5+Mz20/nYyn5gPbeeri5LG9J+Yl430Xpe98nV6U/tlsZOXlyabwXUVJu4HjQAkYiIjlzSiUmbVOhHLfVGxG6W6MiOuctMymjlK0NfRqhKRVkl6WtFPS3SPE50r6S0nPS9ou6ZP1zpnvtGpmk648H5caetUjqR24H7gFuAZYLal2ifLPAi9GxLXASuBLkpITlo83cQXw15KelrQmo+BrJG2RtKWf7KXizSwv1Mwa1/XAzojYFRF9wCPAbTX7BDBbkoBZwGEguRjCeDvnb4iIvZIuBn4k6aWIeHxIiSJ6gB6AOZof47yemU2w8nCIhu8qLpC0pep9T+X//DmLgeon/XuBFTXn+F/ABmAvMBv4rYgYTF10XIkrIvZWvh6Q9F3K2fXx9FFmlmejfFbxUJ3+7ZEyYG0F5kPAc8AHgCspV4KeiIjMZZzG3FSUNFPS7HPfAx8Eto31fGaWH4O0NfRqQC9Qvf7eEso1q2qfBB6Nsp3Aq8DVqZOOp8a1EPhuuVlKB/BnEfHDcZxvyup46+XJeLf+PhnfcXhhMv72+fszY/X+ch4tpef6+vVZLyTjBwfTx+/uW5AZm9mR7vOc1p5e8/HgwJxkvO2s7z2NRXlam6YNQN0MLJN0BfA6cDvw8Zp9XgNuAp6QtBC4CtiVOumYE1dE7AKuHevxZpZfzXrIOiIGJK0FNgHtwPqI2C7pzkp8HfDfgQckvUC5aXlXRBxKndcj581siPLsEM2rrUbERmBjzbZ1Vd/vpdzV1DAnLjMbovzIT76b2U5cZlYj/4/8OHGZ2TCNjIpvJScuMxuiyXcVJ4QT1yQ4e1l6apjegfH9M7QNG8/3pgN96SED1818LRm/p/fDyfjaSx9Lxi/rPJwZe7UjPa1Ne1ty8HTdpdXa/YTZmLmpaGaF4jnnzaxwAhhwjcvMisZNRTMrlnBT0cwK5txEgnnmxGVmw7jGZWaFMsqJBFvCiWsSHL46Pd7oZHQm48dOdacvkBgmVu8X8APT/zEZf+h9S5Pxx7cmp03iU/O2ZMb+ciD9c50eSH8uZyL969t+Nt//+fIqEAOD7pw3s4JxH5eZFUu4qWhmBeM+LjMrJCcuMyuUQJTcOW9mRePOeTMrlHDnvAEcfVt6Ae89/Rcm43NmnEnGT5eyxzutmPtG8tjNZ9NzYtXzwAvvTcb/08oXM2P1lk6b3ZWeUKveg8Bt/cmwJYQTl5kVix+yNrMCco3LzAolAkqDTlxmVjC+q2hmhRK4qWhmhePOeTMroEiP4Gk5J65JMPOtR5Pxl88sSsand6YHJJ0pZf8z3jzjleSxN/3kc8n4Mp5Oxi/70/RYqvYbs+PT2gaSx9ZzarArGVdpXKc/r+W9qVj3gSRJ6yUdkLStatt8ST+S9PPK1wsmtphmNlnKdxXbGnq1SiNXfgBYVbPtbuCxiFgGPFZ5b2ZTRERjr1apm7gi4nGgdh3124AHK98/CHykucUys1aKUEOvVhlrH9fCiNgHEBH7JGU+8CZpDbAGoJsZY7ycmU2WoLVJqRET3kiNiJ6IWB4RyztJLxphZvkQDb5aZayJa7+kRQCVrweaVyQza6mAGFRDr0ZIWiXpZUk7JY3YHy5ppaTnJG2X9NN65xxr4toA3FH5/g7ge2M8j5nlULP6uCS1A/cDtwDXAKslXVOzzzzgT4B/FRHvAD5W77x1+7gkPQysBBZI6gXuAe4Fvi3pU8BrjVzofHbRrJPJ+MG+2cl4vV+Q7vbs8VCz29LHXvXldNkGk1Ho/Jv0OK/+yB5M1VlnoFVfKT1f19GB6cm4x3GNXRPvGF4P7IyIXQCSHqF8c696oraPA49GxGvla0fdFlzdxBURqzNCN9U71syKZ5TPKi6QVL3qb09E9FS9XwzsqXrfC6yoOcfbgE5JfwvMBu6LiIdSF/XIeTMbKoDGE9ehiFieiI90otr6XAfwTylXhqYDP5P0ZERkPvbhxGVmwzSxqdgLLK16vwTYO8I+hyLiJHBS0uPAtUBm4sr3GkRm1gKN3VFs8K7iZmCZpCskdQG3U765V+17wPsldUiaQbkpuSN1Ute4zGy4JtW4ImJA0lpgE9AOrI+I7ZLurMTXRcQOST8EtlK+H/T1iNiWfVYnLjOrFc2dHSIiNgIba7atq3n/B8AfNHpOJ65JcGYg/TG/cSY9HKLepG4Xdx/PjP30dHrKnMGtLyXj4/VsX/aAijal/6y/fnRuMn713P3JeKk7GbYUz8dlZsWT72cVnbjMbLh6I49bzInLzIYa3TiulnDiMrNhPOe8mRWPE5eZFY6bimZWNHVGqrScE9ckOPjL9Dit7o7xLdN12bTaJQHedNfmf5M89kqeHde16/npyaszY/2RnrbmxKGZyfhLcxcm4+EH2sYmBA1OEtgqTlxmNpxrXGZWOE5cZlY4TlxmVigegGpmReS7imZWPE5cZlY0rnEZ/Se6kvFT8zqT8Wnt6XW2/t3cFzJjf77hg8lj62pLj7ViMF22H77xjszYexe8mjy24xfpX8+XOy5Jxlk8vvFx5zX3cZlZoQRuKppZATlxmVnRyBMJmlnhuMZlZkWi8F1FMysi31U0s8JxjcvoT//1mtN1NhlfOONYMt6ZWEpq3rMHk8emR2GBOtO/InE2fYZXX85e13HVJduTx3YeT39uAwvS8c4jdcagWaa8NxXrTrUmab2kA5K2VW37oqTXJT1Xed06scU0s0kT5buKjbxapZE5Ih8AVo2w/SsRcV3ltXGEuJkVVTT4apG6iSsiHgey5wY2s6mn6IkrYa2krZWm5AVZO0laI2mLpC39pPtyzCwfzg2JqPdqlbEmrq8CVwLXAfuAL2XtGBE9EbE8IpZ3Mm2MlzMze9OYEldE7I+IUkQMAl8Drm9uscyspaZiU1FS9T3ujwLbsvY1s4IpwF3FuuO4JD0MrAQWSOoF7gFWSrqOcs7dDXxm4opYfPO2pz/mC689mT6+83Qy/qdH35kZG3x1T/LYukr1RnqlXbYx+7d79YefTx77tZnpucTmXXQiGT9xOLPr1erJ+TiuuokrIlaPsPkbE1AWM8sBkf8BqB45b2bD5TxxeZFyMxuqwaEQjdbKJK2S9LKknZLuTuz3zySVJP1GvXM6cZnZcIMNvuqQ1A7cD9wCXAOslnRNxn7/E9jUSPGcuMxsmCbWuK4HdkbErojoAx4Bbhthv/8AfAc40MhJnbjMbLjGx3EtOPdkTOW1puZMi4HqW9u9lW2/Imkx5WFV6xotnjvnJ8HC//1/k/GB1XOT8bOD6X+mfzLtjczYn//r9JCC2d96MhlH4/vbNvP5vZmx75+4Kn3pOk2Rtrb0DgNzxjeU47w1usGlhyJieSI+0txDtWf/I+CuiChJjU1g6MRlZsM0cThEL7C06v0SoPav2XLgkUrSWgDcKmkgIv4i66ROXGY2XPMS12ZgmaQrgNeB24GPD7lUxBXnvpf0APD9VNICJy4zG0GzHueJiAFJaynfLWwH1kfEdkl3VuIN92tVc+Iys6Ga/AB1ZaLRjTXbRkxYEfE7jZzTicvMhhAj96jniROXmQ2X80d+nLjMbBg/ZG3EwEAyfmqgKxm/dPrR9PGD2TPLnlidPnb2t5Jhor8vvUMdA72vZ8beP2Nn8tjfX5qe6nvBjFPJ+JEz85NxS3DiMrNCidZOEtgIJy4zG841LjMrGvdxmVnxOHGZWdG4xmVmxRI0NElgKzlxmdkQXizDGrJo+rFk/OLO48n4wYE5mbHPXfWT5LHf5pJkfCJd1J7+s37rNduT8Tkd6WXbXum+dNRlsgonLjMrGkW+M5cTl5kN1eTZISaCE5eZDeM+LjMrHD/yY2bF4xqXmRXKKFapbhUnLjMbruiJS9JS4CHgEsrjaXsi4j5J84FvAZcDu4HfjIhfTlxRp66/efodyfh9N/+fZPzZU5dnxl4r1ZuTqnW/oY8ef1sy/s6Zvcn4vPb0fFwPt60YdZmsGANQG1ntcwD4vYh4O/Ae4LOSrgHuBh6LiGXAY5X3ZjYFaDAaerVK3cQVEfsi4pnK98eBHZSX0L4NeLCy24PARyaojGY2mWIUrxYZVR+XpMuBdwFPAQsjYh+Uk5uki5tfPDNrhSkzHELSLOA7wOcj4lhluexGjlsDrAHoZsZYymhmk20K9HEhqZNy0vpmRDxa2bxf0qJKfBFwYKRjI6InIpZHxPJOshd1MLP8UDT2apW6iUvlqtU3gB0R8eWq0Abgjsr3dwDfa37xzGzSBRDR2KtFGmkq3gB8AnhB0nOVbV8A7gW+LelTwGvAxyakhOeBt3/lUDJ+5APpJnZ/tGfGrp6+L3nstl9bmYwPbn0pGR+PV89elIxfMe1gMt7d1p+MdxzxMMWxKnwfV0T8Hdkrct/U3OKYWasVYRyX/ySZ2VAtbgY2wonLzIZxjcvMiseJy8yKxjUuMyuWAEr5zlxOXGY2jGtcVlfp57uS8ZdOp5fZWjwtezahelO/7L/hgmT8oq3J8LgcH+hOxmdMP5uMz2tL/2ylaTn/35dnTbyrKGkVcB/QDnw9Iu6tif9b4K7K2xPAv4+I51PndOIys2GaVeOS1A7cD9wM9AKbJW2IiBerdnsV+BcR8UtJtwA9QHIytYaeVTSz80hzp7W5HtgZEbsiog94hPKUWG9eLuIfqiYhfRJYUu+krnGZ2RAC1Hjn/AJJW6re90RET9X7xcCeqve9pGtTnwJ+UO+iTlxmNswoVrI+FBHLU6caYduIJ5d0I+XE9b56F3XiMrOhmju7aS+wtOr9EmBv7U6Sfg34OnBLRPyi3kndx2VmNRqc0qaxWtlmYJmkKyR1AbdTnhLrVyRdBjwKfCIiXmnkpK5xmdkwzbqrGBEDktYCmygPh1gfEdsl3VmJrwP+G3Ah8CeVmZUH6jQ/nbgmRb1pruv85Xrk79+bjP/nm7LncDxSSs/lpVvr1Mq/mg6Px95Tc5PxrjmlZLxTA+kLtHkc15g1cRxXRGwENtZsW1f1/aeBT4/mnE5cZjZUjOquYks4cZnZcPnOW05cZjbcKIZDtIQTl5kN58RlZoUSQNEXyzCz84sINxXNrIAG813lcuKaBGrPXvcQIAbS45Eu+0H6l6j9X2bH9/enx0otX7gnGd+djI7P3hNzkvH57SeS8efOvCUZ1wV9oy6T4aaimRWTm4pmVjxOXGZWLF4Q1syKxqv8mFkRuY/LzIrHicvMCiWAwYInLklLgYeASyiP7uiJiPskfRH4XeBgZdcvVObdsRpRSs8rVc+0v9qcjP/4v1ydGbtyxqHksTfM+Xkyvuv9H07G2554NhlPOXJ8ejJ+ScfxZPz4YPr4ONI16jIZTJXO+QHg9yLiGUmzgacl/agS+0pE/OHEFc/MWqLoiSsi9gH7Kt8fl7SD8pJDZjYVBVDK99D5US2WIely4F3AU5VNayVtlbRe0ohruUtaI2mLpC39pJdUN7M8CIjBxl4t0nDikjQL+A7w+Yg4Rnk28iuB6yjXyL400nER0RMRyyNieSfTxl9iM5t4zVvlZ0I0dFdRUiflpPXNiHgUICL2V8W/Bnx/QkpoZpOrAHcV69a4VF4v6BvAjoj4ctX2RVW7fRTY1vzimVlLTIEa1w3AJ4AXJD1X2fYFYLWk6yjn593AZyagfFPDBP8DP7NvaWbsrndvSh57MtK/Aq99qDsZv/yJZDhp7qwzyfgl7XWGkXQdSIY7Lzo92iLZOVPgruLfASMtDOgxW2ZTUQSMc+zhRPPIeTMbrug1LjM7DzlxmVmxRO7vKjpxmdlQAdHCwaWNcOIys+Fy/siPE5eZDRXh5cls4i35H9mxX//dzyWPVf9II13edPnfTuASX49emAyvOPgfk/G2o53J+OKf5Ps/X665c97MiiZc4zKzYpkaEwma2fmkAA9ZO3GZ2RDB+Kcbn2ijmkjQzM4D0dyJBCWtkvSypJ2S7h4hLkl/XIlvlfTueud0jcvMhokmNRUltQP3AzcDvcBmSRsi4sWq3W4BllVeKyhPUroidV7XuMxsuObVuK4HdkbErojoAx4BbqvZ5zbgoSh7EphXM9/fMIpJvHsg6SDwj1WbFgDp9bNaJ69ly2u5wGUbq2aW7S0RcdF4TiDph5TL1IhuoHpitZ6I6Kk6128AqyLi05X3nwBWRMTaqn2+D9xbmUILSY8Bd0XElqyLTmpTsfYDlbQlIpZPZhkaldey5bVc4LKNVd7KFhGrmni6kUY419aWGtlnCDcVzWwi9QLVU/QuAfaOYZ8hnLjMbCJtBpZJukJSF3A7sKFmnw3Ab1fuLr4HOFpZzzVTq+8q9tTfpWXyWra8lgtctrHKc9nGJSIGJK0FNgHtwPqI2C7pzkp8HeVp4G8FdgKngE/WO++kds6bmTWDm4pmVjhOXGZWOC1JXPUeAWglSbslvSDpOUmZ40gmqSzrJR2QtK1q23xJP5L088rXC3JUti9Ker3y2T0n6dYWlW2ppJ9I2iFpu6TPVba39LNLlCsXn1uRTHofV+URgFeoegQAWF3zCEDLSNoNLI+Ilg9WlPTPgROURxW/s7Lt94HDEXFvJelfEBF35aRsXwRORMQfTnZ5asq2CFgUEc9Img08DXwE+B1a+NklyvWb5OBzK5JW1LgaeQTAgIh4HDhcs/k24MHK9w9S/sWfdBlly4WI2BcRz1S+Pw7sABbT4s8uUS4bpVYkrsXAnqr3veTrHy+Av5b0tKQ1rS7MCBaeG+NS+Xpxi8tTa23lCf/1rWrGVpN0OfAu4Cly9NnVlAty9rnlXSsS16iH90+yGyLi3ZSfWP9spUlkjfkqcCVwHbAP+FIrCyNpFvAd4PMRcayVZak2Qrly9bkVQSsS16iH90+miNhb+XoA+C7lpm2e7D/35Hzl64EWl+dXImJ/RJSivCjf12jhZyepk3Jy+GZEPFrZ3PLPbqRy5elzK4pWJK5GHgFoCUkzK52mSJoJfBDYlj5q0m0A7qh8fwfwvRaWZYiaqUg+Sos+O0kCvgHsiIgvV4Va+tlllSsvn1uRtGTkfOV27x/x5iMAiQW2Jo+kt1KuZUH5cag/a2XZJD0MrKQ8xch+4B7gL4BvA5cBrwEfi4hJ7yTPKNtKys2dAHYDn6n3zNkEle19wBPAC8C5SaO+QLk/qWWfXaJcq8nB51YkfuTHzArHI+fNrHCcuMyscJy4zKxwnLjMrHCcuMyscJy4zKxwnLjMrHD+P8n0b3NWXtVoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create a new window/container/figure to draw in\n",
    "plt.figure()\n",
    "\n",
    "#displays the image located at index 5 of the X test array\n",
    "plt.imshow(image_data)\n",
    "\n",
    "#adds a colour bar to the plot\n",
    "plt.colorbar()\n",
    "\n",
    "#Turns off grid lines\n",
    "plt.grid(False)\n",
    "\n",
    "#Displays entire plot or entire visualization and without it, the plot would not be shown\n",
    "#note this is not a traditional photograph but rather a representation of pixel data.\n",
    "#The function imshow inteprets numerical values in the array as intensity values and assigns colours accordingly\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1153393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You need to reshape the image into something similar to an actual photograph using cv2. .reshape(height, width, channels.)\n",
    "#RGB = 3 CHANNELS AND grey = one channel\n",
    "image = image_data.reshape(28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a06b3ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReshaped Image\u001b[39m\u001b[38;5;124m'\u001b[39m, image)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#cv2.waitKey(delay): This is for leaving the image window open until a key is pressed. After the key is pressed,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#it waits for the delay(millseconds) until it closes the window\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#used to destroy all OpenCV windows that are currently open\u001b[39;00m\n\u001b[1;32m      8\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#This opens a new window with the title reshaped image\n",
    "cv2.imshow('Reshaped Image', image)\n",
    "#cv2.waitKey(delay): This is for leaving the image window open until a key is pressed. After the key is pressed,\n",
    "#it waits for the delay(millseconds) until it closes the window\n",
    "cv2.waitKey(0)\n",
    "\n",
    "#used to destroy all OpenCV windows that are currently open\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78581453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You have seen earlier that the training and test set is currently in the form of (samples x height x width). \n",
    "#However, a CNN in TensorFlow Keras expects input training images in the format (height x width x number of colour \n",
    "#channels). Currently, we do not have a colour channel axis in our input images. The following script adds colour \n",
    "#channel axes to the images in the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0abd05b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "#np.expand_dims adds another dimension to an array. If it was 2D, its now 3D. i.e (if it was row x col) its now (row by col by height)\n",
    "#array is not necessarily a matrix\n",
    "#-1 just means add a new dimension at the end of the array and the entries of that dimension is 1\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de8e1324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "#just to see how the shape of one image is looking\n",
    "print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "461e4531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "#label encoding\n",
    "#labels are represented as labels and set extracts the unique labels from y_train\n",
    "#y_train is one dimensional array containing the labels of all the information\n",
    "#There should be a key else where to determine what each label correspond to.\n",
    "outputs = set(y_train)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46d67b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are going to define our model.\n",
    "\n",
    "#This layer is defined with the shape of the input data. \n",
    "#So you can just take one image and pass in its shape as the shape argument of the Input functions\n",
    "inp_layer = Input(shape = X_train[0].shape )\n",
    "\n",
    "#convulutional layers\n",
    "#This layer is the first convulutional layer with 32 filters of 3by3. This just means 32 different matrices of size 3x3\n",
    "#At one time, each matrix/filter acts as a feature detector and slides/convolves accross the input data to produce feature maps\n",
    "#Stride defines the number of pixels by which the filter shifts\n",
    "#activation introduce non-linearity\n",
    "#note the syntax. We basically sauing lets apply the convulution to the input layer and we get feature maps\n",
    "conv1 = Conv2D(32, (3,3), strides = 2, activation= 'relu')(inp_layer)\n",
    "\n",
    "#downsampling operation that reduces spatial dimensions(width and height) of the feature maps using a 2 x 2 pooling window\n",
    "#The result is a new set of downsample feature maps and eachmap has half the spatial dimenstions(width and height) compared to the input feature maps\n",
    "#Max-pooling helps in reducing the spatial resolution, retaining the most important information, and making the subsequent layers focus on more significant features.\n",
    "#Its called max pooling coz it takes the maximum in each window\n",
    "maxp1 = MaxPool2D(2, 2)(conv1)\n",
    "\n",
    "#convolution layer 2 doing the same thing as conv1. Only diff is that we have 64 filters\n",
    "#Increasing the number of filters in deeper layers allows the network to capture more complex and abstract \n",
    "#features. However, it also increases the computational cost. It's a common practice to gradually increase the \n",
    "#number of filters in deeper layers to capture higher-level representations of the input data\n",
    "conv2 = Conv2D(64, (3,3), strides = 2, activation= 'relu')(maxp1)\n",
    "\n",
    "#Pooling layers, such as MaxPooling or AveragePooling, are commonly used in CNNs to reduce the spatial dimensions (width and height) of the feature maps, which helps in controlling overfitting, reducing computational complexity, and increasing the receptive field.\n",
    "#In many architectures, after convolutional layers, there's a trade-off between retaining spatial information \n",
    "#and reducing dimensionality. The absence of pooling after certain convolutional layers might imply that the \n",
    "#model is trying to preserve more spatial information at that stage, allowing the subsequent layers to work with \n",
    "#more detailed feature maps.\n",
    "\n",
    "#Instead of pooling, the code you mentioned might directly flatten the output of the last convolutional layer (conv2) to feed it into fully connected layers (Dense). This could be a design choice aimed at preserving spatial information up to a certain depth before transitioning to fully connected layers for high-level reasoning and classification.\n",
    "\"\"\"conv2: This represents the output tensor or feature map produced by the second convolutional layer in your neural network. The convolutional layers learn hierarchical features from the input data.\n",
    "Flatten(): This is a layer in the neural network architecture responsible for converting the multi-dimensional tensor or feature map into a one-dimensional array. It essentially flattens the spatial dimensions of the input.\n",
    "(conv2): The (conv2) part is a functional API syntax in Keras (a high-level neural networks API running on top of TensorFlow). It means applying the Flatten layer to the output of conv2.\n",
    "So, the line flat1 = Flatten()(conv2) takes the output of the second convolutional layer (conv2), which is a multi-dimensional tensor, and flattens it into a one-dimensional array. This flattened array can then be used as input to subsequent layers, such as fully connected (dense) layers, in your neural network.\n",
    "Flattening is often necessary when transitioning from convolutional layers to dense layers, as fully connected layers require one-dimensional input. The flattened representation serves as a feature vector that can be used for further processing or classification tasks.\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"A tensor is a mathematical object that generalizes the concept of scalars, vectors, and matrices. In the context of deep learning and neural networks, tensors usually refer to multi-dimensional arrays. A tensor can be a scalar (0D tensor), a vector (1D tensor), a matrix (2D tensor), or have more dimensions.\n",
    "A feature map, on the other hand, is a term commonly used in the context of convolutional neural networks (CNNs). It refers to the output of a layer in a neural network that performs convolutional operations. In image processing tasks, a feature map can be thought of as a set of learned filters or patterns that the network has detected in the input data.\"\"\"\n",
    "flat1 = Flatten()(conv2)\n",
    "\n",
    "\"\"\"Dense(256, activation='relu'): This part defines a dense layer with 256 units/neurons. The term \"dense\" implies that each neuron in this layer is connected to every neuron in the previous layer (which is flat1 in this case). The number 256 represents the dimensionality of the output space, meaning there will be 256 neurons in this layer.\n",
    "flat1: This is the input to the dense layer. It's the flattened output from the previous layer, which is usually the output of a convolutional layer. Flattening means converting the multi-dimensional tensor (or feature map) into a one-dimensional tensor.\n",
    "(flat1): The (flat1) at the end is actually a function call. In Keras (a high-level neural networks API running on top of TensorFlow), this functional API is used. The layer is applied to the input tensor flat1 by calling it as a function. This is a concise way of defining a layer and connecting it to the previous layer.\n",
    "activation='relu': The ReLU (Rectified Linear Unit) activation function is applied element-wise to the output of this dense layer. ReLU introduces non-linearity to the model, helping it learn complex patterns.\"\"\"\n",
    "dense1 = Dense(256, activation = 'relu')(flat1)\n",
    "\n",
    "#same with dense1. Dense(num of neurons in this layer, activation function)\n",
    "dense2 = Dense(128, activation = 'relu')(dense1)\n",
    "\n",
    "#same but here the num of neurons should be the same as the number of the labels\n",
    "out_layer = Dense(len(outputs), activation= 'softmax')(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45444e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This defines an instance of the Model class taking in the input and output layers of the model.\n",
    "#I think taking in the output layer, it traverses back until it reaches the input layer\n",
    "#Here, a Keras functional API model is created by specifying the input layer (inp_layer) and the output\n",
    "#layer (out_layer). This model represents the architecture you defined earlier with convolutional and dense layers.\n",
    "model_func = Model(inp_layer, out_layer)\n",
    "\n",
    "#The model is compiled with specific configurations for training. It uses the Adam optimizer, sparse categorical \n",
    "#crossentropy as the loss function (suitable for integer-encoded labels), and accuracy as the metric to monitor during training.\n",
    "\n",
    "model_func.compile(optimizer = 'adam', loss= 'sparse_categorical_crossentropy', metrics =['accuracy'])\n",
    "\n",
    "#And the script below displays  the model architecture.\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\"\"\"The model is compiled with specific configurations for training. It uses the Adam optimizer, sparse \n",
    "categorical crossentropy as the loss function (suitable for integer-encoded labels), and accuracy as the\n",
    "metric to monitor during training.\"\"\"\n",
    "\n",
    "plot_model(model_func, to_file='model_plot.png', show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a027c0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3750/3750 [==============================] - 13s 3ms/step - loss: 0.5256 - accuracy: 0.8051 - val_loss: 0.4318 - val_accuracy: 0.8369\n",
      "Epoch 2/5\n",
      "3750/3750 [==============================] - 10s 3ms/step - loss: 0.3776 - accuracy: 0.8594 - val_loss: 0.3728 - val_accuracy: 0.8654\n",
      "Epoch 3/5\n",
      "3750/3750 [==============================] - 12s 3ms/step - loss: 0.3363 - accuracy: 0.8744 - val_loss: 0.3484 - val_accuracy: 0.8692\n",
      "Epoch 4/5\n",
      "3750/3750 [==============================] - 11s 3ms/step - loss: 0.3063 - accuracy: 0.8855 - val_loss: 0.3516 - val_accuracy: 0.8746\n",
      "Epoch 5/5\n",
      "3750/3750 [==============================] - 11s 3ms/step - loss: 0.2843 - accuracy: 0.8932 - val_loss: 0.3311 - val_accuracy: 0.8789\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"The fit method is used to train the model on the provided training data. Let's break down the parameters used in this specific call:\n",
    "\n",
    "X_train: The input training data (images in this case).\n",
    "y_train: The labels corresponding to the training data.\n",
    "epochs=5: The number of times the model will iterate over the entire training dataset. In this case, it's set to 5 epochs.\n",
    "batch_size=16: The number of samples per gradient update. The model weights are updated after each batch of 16 samples.\n",
    "validation_data=(X_test, y_test): Data on which to evaluate the loss and any model metrics at the end of each epoch. In this case, it's the test set.\n",
    "The fit method returns a History object, which contains information about the training process. This includes the loss and accuracy on the training and validation datasets for each epoch.\n",
    "\"\"\"\n",
    "history = model_func.fit(X_train,\n",
    "y_train,\n",
    "epochs=5,\n",
    "batch_size=16,\n",
    "validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ba61a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
